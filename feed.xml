<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bobb1ranger.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bobb1ranger.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-19T20:43:03+00:00</updated><id>https://bobb1ranger.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Kalman Filter Recap</title><link href="https://bobb1ranger.github.io/blog/2026/KalmanFilter/" rel="alternate" type="text/html" title="Kalman Filter Recap"/><published>2026-01-19T10:44:00+00:00</published><updated>2026-01-19T10:44:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2026/KalmanFilter</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2026/KalmanFilter/"><![CDATA[<h1 id="1-math-formulation-of-the-kalman-filter">1. Math Formulation of the Kalman Filter:</h1> <p>Consider the dynamical system:</p> \[\begin{align*} x_{k+1} &amp; = A_k x_k + B_k u_k + w_k \\ z_k &amp; = C_k x_k + v_k \end{align*}\] <p>where \(x_k\) is the state vector at time step \(k\), \(u_k\) is the control input, \(z_k\) is the measurement vector, and \(w_k\) and \(v_k\) are process and measurement noise, respectively. The noise terms are assumed to be zero-mean Gaussian with covariances \(Q_k\) and \(R_k\).</p> <p>A priori estimate (prediction):</p> \[\begin{align*} \hat{x}_{k|k-1} &amp; = A_{k-1} \hat{x}_{k-1|k-1} + B_{k-1} u_{k-1} \\ P_{k|k-1} &amp; = A_{k-1} P_{k-1|k-1} A_{k-1}^T + Q_{k-1} \end{align*}\] <p>The innovation (prefit residual) and its covariance are given by:</p> \[\begin{align*} \hat{y}_{k} = z_k - C_k \hat{x}_{k|k-1} \\ S_k = C_k P_{k|k-1} C_k^T + R_k \end{align*}\] <p>Intuitively, the innovation represents the discrepancy between the actual measurement and the predicted measurement based on the a priori state estimate.</p> <p>Kalman gain:</p> \[K_k = P_{k|k-1} C_k^T S_k^{-1}\] <p>The gain balances the trust between the a priori estimate and the new measurement. Intuitively:</p> <ul> <li>If the measurement noise covariance \(R_k\) is small (i.e., measurements are reliable), the Kalman gain will be higher, giving more weight to the measurement.</li> <li>If the priori estimate covariance \({P}_{k \vert k-1}\) is small (i.e., the model prediction is reliable), the Kalman gain will be lower, giving more weight to the model prediction.</li> </ul> <p>A posteriori estimate (update):</p> \[\begin{align*} \hat{x}_{k|k} &amp; = \hat{x}_{k|k-1} + K_k \hat{y}_k = (I - K_k C_k) \hat{x}_{k|k-1} + K_k z_k, \\ P_{k|k} &amp; = (I - K_k C_k) P_{k|k-1} = (I - K_k C_k) P_{k|k-1} (I - K_k C_k)^T + K_k R_k K_k^T. \end{align*}\] <p>The posteriori covariance matrix \({P}_{k \vert k}\) can also be given as:</p> \[P_{k|k} = P_{k \vert k-1} - P_{k \vert k-1} C_k^T (C_k P_{k \vert k-1} C_k^T + R_k)^{-1} C_k P_{k \vert k-1},\] <p>which leads to the priori covariance update as a discrete-time Riccati equation:</p> \[\begin{equation}\label{eq:riccati} P_{k+1|k} = A_k P_{k \vert k-1} A_k^T + Q_k - A_k P_{k \vert k-1} C_k^T (C_k P_{k \vert k-1} C_k^T + R_k)^{-1} C_k P_{k \vert k-1} A_k^T. \end{equation}\] <p>This Riccati equation is independent of the control input matrix \(B_k\), since the estimation error dynamics do not depend on the control inputs in linear systems. (When the system if time-invariant, the steady-state solution can be found by solving the algebraic Riccati equation.)</p> <hr/> <h1 id="2-optimality-of-the-kalman-filter">2. Optimality of the Kalman Filter:</h1> <p>The optimality of the Kalman filter is very strong, but also conditional.</p> <h2 id="21-optimal-in-the-mmse-sense">2.1 Optimal in the MMSE sense:</h2> <p>The Kalman filter produces the estimate \(\hat{x}_{k|k}\) that minimizes the mean squared error (MMSE) between the true state \(x_k\) and the estimate \(\hat{x}_{k|k}\), given all measurements up to time step \(k\).</p> \[\begin{align*} \hat{x}_{k|k} &amp;= \mathbb{E}[x_k | z_1, z_2, \ldots, z_k] \\ \hat{x}_{k|k} &amp;= \arg\min_{\tilde{x}} \mathbb{E} \left[ \| x_k - \tilde{x} \|^2 \mid z_1, z_2, \ldots, z_k \right] \end{align*}\] <p>When the system involves multivariate states and measurements, the Kalman filter minimizes the trace of the error covariance matrix:</p> \[\hat{x}_{k|k} = \arg\min_{\tilde{x}} \text{trace} \left( \mathbb{E} \left[ (x_k - \tilde{x})(x_k - \tilde{x})^T \mid z_1, z_2, \ldots, z_k \right] \right) = \arg\min_{\tilde{x}} \text{trace}(P_{k|k})\] <p>This optimality holds under the assumptions of linearity and Gaussian noise.</p> <ul> <li>The system is linear.</li> <li>\(w_k\) and \(v_k\) are zero-mean Gaussian noise with known covariances.</li> <li>The initial state \(x_0\) is Gaussian with known mean and covariance.</li> </ul> <h2 id="22-exact-bayesian-filter-for-lineargaussian-systems">2.2 Exact Bayesian filter for linear–Gaussian systems:</h2> <p>Under these assumptions, one can show that the Kalman filter is equivalent to the optimal Bayesian filter for linear–Gaussian systems. The posterior distribution of the state given the measurements is Gaussian, and the Kalman filter provides the exact mean and covariance of this distribution.</p> <ul> <li>The posterior distribution \({p}(x_k \vert z_1, z_2, \ldots, z_k)\) is Gaussian.</li> <li>The Kalman filter computes the mean and covariance of this posterior distribution exactly.</li> <li>No approximation is involved.</li> </ul> <h2 id="23-best-linear-unbiased-estimator">2.3 Best Linear Unbiased Estimator:</h2> <p>Even without Gaussianity, the Kalman filter is still optimal among all linear unbiased estimators. KF minimmizes the estimation error covariance among all linear estimators that are unbiased. This is the given by the Gauss–Markov theorem.</p> <hr/> <h1 id="3-extended-kalman-filter-ekf">3. Extended Kalman Filter (EKF):</h1> <p>For nonlinear systems, the Extended Kalman Filter (EKF) linearizes the system around the current estimate to apply the Kalman filter framework. The EKF approximates the state transition and measurement functions using their Jacobians.</p> \[\begin{align*} x_k &amp; = f(x_{k-1}, u_{k-1}) + w_{k-1} \\ z_k &amp; = h(x_k) + v_k \end{align*}\] <p>Predict:</p> \[\begin{align*} \hat{x}_{k|k-1} &amp; = f(\hat{x}_{k-1|k-1}, u_{k-1}) \\ P_{k|k-1} &amp; = F_{k-1} P_{k-1|k-1} F_{k-1}^T + Q_{k-1};\quad F_{k-1} = \left. \frac{\partial f}{\partial x} \right|_{x = \hat{x}_{k-1|k-1}, u = u_{k-1}} \end{align*}\] <p>Innovate:</p> \[\begin{align*} \hat{y}_k &amp; = z_k - h(\hat{x}_{k|k-1}) \\ S_k &amp; = H_k P_{k|k-1} H_k^T + R_k;\quad H_k = \left. \frac{\partial h}{\partial x} \right|_{x = \hat{x}_{k|k-1}} \end{align*}\] <p>Kalman Gain:</p> \[K_k = P_{k|k-1} H_k^T S_k^{-1}\] <p>Update:</p> \[\begin{align*} \hat{x}_{k|k} &amp; = \hat{x}_{k|k-1} + K_k \hat{y}_k \\ P_{k|k} &amp; = (I - K_k H_k) P_{k|k-1} \end{align*}\] <p>There is no optimality guarantee for EKF in general, but it works well in practice for many applications. For slowly varying nonlinear systems, EKF can provide good approximations to the optimal Bayesian filter.</p> <hr/> <h1 id="4-relation-to-optimal-state-feedback-control">4. Relation to Optimal State-Feedback Control:</h1> <p>Define the estimation error:</p> \[e_k = x_k - \hat{x}_{k|k}\] <p>Consider a posteriori estimate law \(q_k\):</p> \[\hat{x}_{k|k} = \hat{x}_{k|k - 1} + q_k\] <p>Then</p> \[e_k = x_k - \hat{x}_{k|k - 1} - q_k = A_{k-1} e_{k-1} + w_{k-1} - q_k\] <p>What we measure is</p> \[\hat{y}_k = z_k - C_k \hat{x}_{k|k-1} = C_k e_k + v_k\]]]></content><author><name></name></author><category term="control"/><category term="math"/><category term="optimization"/><summary type="html"><![CDATA[A recap of the Kalman Filter algorithm for state estimation. It also covers how it is related to the optimal state-feedback control problem.]]></summary></entry><entry><title type="html">Reinforcement Learning Tech Tree</title><link href="https://bobb1ranger.github.io/blog/2026/Reinforcement-Learning-TechTree/" rel="alternate" type="text/html" title="Reinforcement Learning Tech Tree"/><published>2026-01-12T02:09:00+00:00</published><updated>2026-01-12T02:09:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2026/Reinforcement-Learning-TechTree</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2026/Reinforcement-Learning-TechTree/"><![CDATA[<p><a href="/assets/html/rl_tech_tree.html">View the interactive RL Tech Tree</a></p> <p>I’ll keep updating this as I learn more about reinforcement learning and its various subfields and techniques. If you have suggestions for additions or corrections, feel free to reach out!</p>]]></content><author><name></name></author><category term="ML"/><category term="AI"/><category term="reinforcement-learning"/><summary type="html"><![CDATA[This is a little tech tree I've made to understand the landscape of reinforcement learning.]]></summary></entry><entry><title type="html">ROS 2 Essentials</title><link href="https://bobb1ranger.github.io/blog/2025/ROS2essential/" rel="alternate" type="text/html" title="ROS 2 Essentials"/><published>2025-11-23T02:59:00+00:00</published><updated>2025-11-23T02:59:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/ROS2essential</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/ROS2essential/"><![CDATA[<p>A quick Recap on why ROS 2 is invented to improve upon ROS.</p> <h1 id="i-why-ros-2">I Why ROS 2</h1> <table> <thead> <tr> <th>Feature</th> <th>ROS</th> <th>ROS 2</th> </tr> </thead> <tbody> <tr> <td>Computational Resources</td> <td>Strong, local resources</td> <td>Potentially limited locally</td> </tr> <tr> <td>Number of Agents</td> <td>Few agents</td> <td>Hundreds of agents</td> </tr> <tr> <td>Network Reliability</td> <td>Stable and reliable</td> <td>Not always reliable</td> </tr> <tr> <td>Application</td> <td>Research-oriented</td> <td>Commercial and industrial use</td> </tr> </tbody> </table> <hr/> <h2 id="1-limitations-of-ros">1. Limitations of ROS:</h2> <ul> <li>ROS is no longer used exclusively for scientific research.</li> <li>More ROS-based products are being commercialized.</li> <li>The increasing variety of application scenarios introduces additional requirements for the system.</li> </ul> <h2 id="2-various-demands-in-the-era-of-intelligent-robotics">2. Various demands in the era of intelligent robotics:</h2> <ul> <li>Streamlined methods for multi-robot systems</li> <li>heterogeneous system compatibility</li> <li>Real-time performance capabilities</li> <li>Robustness to changing network conditions</li> <li>Better suitability for commercial products</li> <li>Lifetime project management</li> </ul> <hr/> <h1 id="ii-main-differences">II Main differences</h1> <h2 id="1-a-revolution-on-architecture">1. A revolution on architecture</h2> <p>In ROS, all nodes are managed by a single node called “Master”. This increases the risk of single point failure. In ROS2, the discovery mechanism based on DDS mitigates the risk.</p> <ul> <li>Date Distribution Service (DDS): A middleware standard that enables real-time, decentralized communication between applications.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Blogimgs1/ROS1-ROS2-architecture-480.webp 480w,/assets/img/Blogimgs1/ROS1-ROS2-architecture-800.webp 800w,/assets/img/Blogimgs1/ROS1-ROS2-architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Blogimgs1/ROS1-ROS2-architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The architectures of ROS and ROS 2. </div> <h2 id="2-re-design-of-the-api">2. Re-design of the API</h2> <p>ROS 2 adopts the latest C++ standards and Python 3 while retaining most of the familiar usage patterns.</p> <h2 id="3-build-process-upgrade">3. Build-process Upgrade</h2> <p>In ROS 2, the build process is based on ament and colcon, replacing the older rosbuild and catkin systems used in ROS 1. Ament provides a more modular and extensible build framework, while colcon acts as a higher-level build tool that supports parallel builds, better dependency handling, and cleaner workspace management. Together, they offer a more reliable and scalable workflow, especially for large, multi-package ROS projects.</p>]]></content><author><name></name></author><category term="robotics"/><category term="simulation"/><category term="programming"/><summary type="html"><![CDATA[How ROS 2 is different from ROS.]]></summary></entry><entry><title type="html">Mujoco</title><link href="https://bobb1ranger.github.io/blog/2025/Mujoco_notes/" rel="alternate" type="text/html" title="Mujoco"/><published>2025-11-15T01:14:00+00:00</published><updated>2025-11-15T01:14:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/Mujoco_notes</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/Mujoco_notes/"><![CDATA[<h1 id="mujoco-overview">MuJoCo Overview</h1> <p>MuJoCo is a full-featured simulator designed from the ground up for the purpose of <strong>model-based</strong> optimization, and in particular optimization through contacts. Because of its efficiency and high accuracy, MuJoCo makes it possible to evaluate many computationally intensive techniques—such as optimal control, physically consistent state estimation, and system identification—on complex dynamical systems with <strong>contact-rich</strong> behaviors. It also supports more traditional applications, including validating control schemes before deploying them on physical robots, interactive scientific visualization, virtual environments.</p> <p>Key features:</p> <ul> <li>Simulation in generalized coordinates, avoiding joint violations</li> <li>Inverse dynamics that are well-defined <strong>even in the presence of contacts</strong>.</li> <li>Kinematic constraints treatment.</li> <li>Many types of available actuators.</li> <li>Vast options of ODE solvers and integrators.</li> <li>XML model format (called MJCF) and built-in model compiler. Cross-platform GUI in OpenGL.</li> </ul> <h1 id="cmake-and-its-use-in-mujoco">Cmake and its use in MujoCo</h1> <h2 id="what-is-cmake">What is CMake</h2> <p>CMake is a cross-platform build system generator.</p> <ul> <li>reads configuration files (CMakeLists.txt)</li> <li>Detects the system, compilers, dependencies and paths.</li> <li>Generates platform -specific build systems. For example, Makefiles on Linux.</li> </ul> <p>CMake = the tool that prepares the actual build environment, so you can build the same project across many platforms without rewriting build scripts.</p> <h2 id="cmakes-functrelated_posts-false">CMake’s functrelated_posts: false</h2> <ul> <li> <p>“/blog/2025/Mujoco_notes”ion in Mujoco Mujoco uses CMake to configure, build and install its source code. Specifically, CMake handles several things.</p> </li> <li>Building and configuring the Mujoco library. (If we’re using precompiled Mujoco binaries, we don’t need CMake.)</li> <li>Managing dependencies. (GL/GLEW)</li> <li>Cross-platform support.</li> <li>Configuring compile options.</li> <li>Installation and packaging. (Let CMake install headers and libraries to a clean, standard layout.)</li> </ul> <p>Convert Solidworks drawings to MJCF related_posts: false</p> <ul> <li>“/blog/2025/Mujoco_notes”</li> </ul> <h2 id="file-type-and-simulation-building-process">File Type and Simulation Building Process</h2> <h3 id="model-format">Model Format:</h3> <p>.xml type file (Reference: Mujoco official site) Note .xml is super easy to program.</p> <h3 id="compiling">Compiling</h3> <h3 id="visualization-node">Visualization Node</h3> <p>—related_posts: false</p> <ul> <li>“/blog/2025/Mujoco_notes” <h3 id="root-node">Root Node:</h3> <p>Compiler node: used for simulation computations.</p> </li> <li>range of physical variables</li> </ul> <p>Option node: simulation configuration.</p> <ul> <li>Time steps (Default 0.002)</li> <li>Gravity (Exogenous forces)</li> <li>Air drag/ wind environment. Air density and viscousity. (Mujoco is mainly used for multi-body kinetic simulation)</li> <li>Integrator. Euler is quick, RK4(Runge-Kutta) is accurate.</li> <li>solver</li> </ul> <p>The difference between integrator and solver:</p> <ul> <li>The integrator is responsible for numerical time integration of the system’s equations of motion.It uses the current state and computed accelerations/velocities to determine the state at the next time step.</li> <li>The solver is a lower-level component used by the integrator (primarily the semi-implicit Euler variants) to resolve forces and impulses arising from constraints.</li> </ul> <p>Visual node. Lighting conditions and Rendering options. Visual map affects the Mouse interactions. Don’t need to change in most scenarios.</p> <h2 id="resources-allocation">Resources Allocation:</h2> <p>asset: construct by vertices/ .obj/ .stl.</p> <ul> <li>texture. can load .png as texture.</li> <li>Only “skybox” texture can be directly loaded. All other textures should be instead called in materials.</li> <li> <hfield>: Terrain features. Usually, it's loaded from files. </hfield> </li> </ul> <p>Difference between URDF and MJCF:</p>]]></content><author><name></name></author><category term="robotics"/><category term="simulation"/><category term="programming"/><summary type="html"><![CDATA[Some basic features of Mujoco.]]></summary></entry><entry><title type="html">A note on Diffusion Model (Credit Qingyuan Jiang)</title><link href="https://bobb1ranger.github.io/blog/2025/DiffusionModel/" rel="alternate" type="text/html" title="A note on Diffusion Model (Credit Qingyuan Jiang)"/><published>2025-11-11T22:47:00+00:00</published><updated>2025-11-11T22:47:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/DiffusionModel</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/DiffusionModel/"><![CDATA[<p>This note is shared with permission from my friend, Qingyuan Jiang.</p> <p>The blog post can be found <a href="https://qingyuan-jiang.github.io/blog/2025/diffusion/">here</a>. You can visit his <a href="https://qingyuan-jiang.github.io/">personal website</a> for more information.</p> <p><em>All credit for the content and ideas in this blog goes to Qingyuan Jiang.</em></p>]]></content><author><name></name></author><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[A Study Note Adapted shared from Qingyuan Jiang's Work.]]></summary></entry><entry><title type="html">Time-domain game interpretation of optimal control</title><link href="https://bobb1ranger.github.io/blog/2025/HinfminmaxGame/" rel="alternate" type="text/html" title="Time-domain game interpretation of optimal control"/><published>2025-11-10T21:06:00+00:00</published><updated>2025-11-10T21:06:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/HinfminmaxGame</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/HinfminmaxGame/"><![CDATA[<h1 id="1-the-game-setup">1. The “Game” Setup</h1> <p>We define the <strong>system</strong>, the <strong>players</strong>, and the <strong>rules</strong>.</p> <h2 id="system-dynamics">System Dynamics</h2> \[x_{k+1} = A x_k + B_1 w_k + B_2 u_k.\] <p>We use a standard LQR-like output for simplicity so that \(\|z_k\|_2^2 = x_k^T Q x_k + u_k^T R u_k\):</p> \[z_k = C_1 x_k + D_{12} u_k = \begin{bmatrix} Q^{1/2} x_k \\ R^{1/2} u_k \end{bmatrix}\] <h2 id="players">Players</h2> <ul> <li><strong>Controller</strong> (\(u_k\)): tries to minimize the cost.</li> <li><strong>Disturbance</strong> (\(w_k\)): tries to maximize the cost.</li> </ul> <h2 id="the-games-objective">The Game’s Objective</h2> <p>The \(H_\infty\) goal is to ensure the energy gain is bounded: \(\sum_{k=0}^{\infty} \|z_k\|_2^2 &lt; \gamma^2 \sum_{k=0}^{\infty} \|w_k\|_2^2\). Define:</p> \[J = \sum_{k=0}^{\infty} \left( \|z_k\|_2^2 - \gamma^2 \|w_k\|_2^2 \right)\] <p>This can be rewritten as a <strong>zero-sum game</strong>:</p> <ul> <li>The controller \(u_k\) tries to <strong>minimize</strong> the score.</li> <li>The disturbance \(w_k\) tries to <strong>maximize</strong> it.</li> </ul> <hr/> <h1 id="2-the-scorecard-dynamic-programming">2. The “Scorecard” (Dynamic Programming)</h1> <p>We apply <strong>dynamic programming</strong> to find the “value” of the game.</p> <h2 id="value-function">Value Function</h2> <p>Define the <strong>cost-to-go</strong>:</p> \[V(x_k) = x_k^T P x_k\] <h2 id="bellman-equation">Bellman Equation</h2> \[V(x_k) = \min_{u_k} \max_{w_k} \left\{ \|z_k\|_2^2 - \gamma^2 \|w_k\|_2^2 + V(x_{k+1}) \right\}\] <p>Substitute the known quantities: <span style="color:red">Why the game cost is formulated as a min-max rather than a max-min program?</span></p> <p>When analyzing the discrete-time or continuous-time \(H_\infty\) control problem, the dynamic programming approach leads to a <strong>min–max optimization</strong>: the controller minimizes cost while the disturbance maximizes it.<br/> A natural question is: <em>what happens if we flip the order to max–min?</em></p> <h3 id="1-meaning-of-changing-the-order">1. Meaning of Changing the Order</h3> <ol> <li> <p><strong>Standard formulation (min–max):</strong></p> <p>The controller picks \(u_k\) to minimize the worst-case effect of \(w_k\). This corresponds to the full information control:</p> \[V(x_k) = \min_{u_k} \max_{w_k} \left\{ \ell(x_k,u_k,w_k) + V(x_{k+1}) \right\}.\] </li> <li> <p><strong>Flipped formulation (max–min):</strong></p> <p>The controller must decide first (announce \(u_k\) or even a strategy), and the disturbance reacts after observing it. This means the controller don’t know about the disturbance at current step:</p> \[V(x_k) = \max_{w_k} \min_{u_k} \left\{ \ell(x_k,u_k,w_k) + V(x_{k+1}) \right\}.\] </li> </ol> <p>If the cost function is <strong>convex in \(u\)</strong> and <strong>concave in \(w\)</strong>, then the two formulations are equivalent due to the <em>minimax theorem</em> (saddle point exists). Otherwise, the order matters — typically, the controller is worse off when forced to move first (a Stackelberg-type disadvantage).</p> <h3 id="2-quadratic-stage-cost">2. Quadratic Stage Cost</h3> <p>For the standard (H_\infty) setting, define</p> \[L(u,w) = x^\top Q x + u^\top R u - \gamma^2 w^\top w + (A x + B_1 w + B_2 u)^\top P (A x + B_1 w + B_2 u).\] <p>Here (P) is the value function matrix at the next step in the dynamic programming recursion.</p> <h3 id="3-convexconcave-isaacs-condition">3. Convex–Concave (Isaacs) Condition</h3> <p>Compute the Hessians with respect to \(u\) and \(w\):</p> \[\frac{\partial^2 L}{\partial u^2} = 2(R + B_2^\top P B_2), \qquad \frac{\partial^2 L}{\partial w^2} = -2(\gamma^2 I - B_1^\top P B_1).\] <p>Thus:</p> <ul> <li>\(L\) is <strong>convex in \(u\)</strong> if \(R + B_2^\top P B_2 \succ 0\).</li> <li>\(L\) is concave in \(w\) if \(\gamma^2 I - B_1^\top P B_1 \succ 0\).</li> </ul> <p>This last inequality is the <strong>Isaacs condition</strong> (or the saddle-point condition). When it holds, we can swap the order of optimization:</p> \[\min_u \max_w L(u,w) = \max_w \min_u L(u,w),\] <p>and the \(H_\infty\) Riccati equation derived via dynamic programming remains valid.</p> <h3 id="4-intuitive-interpretation">4. Intuitive Interpretation</h3> <ul> <li> <p>If the Isaacs condition holds, the disturbance’s energy is bounded by \(\gamma\), ensuring that the problem has a well-defined saddle point.<br/> The order of min–max does not matter.</p> </li> <li> <p>If the condition fails, \(L\) is no longer concave in \(w\); the disturbance can exploit the controller’s pre-commitment, and<br/> the <strong>max–min value</strong> (controller as leader) becomes larger — meaning worse performance for the controller.</p> </li> </ul> <p>This is analogous to the <strong>Stackelberg vs. Nash</strong> distinction in game theory.</p> <h3 id="5-practical-check">5. Practical Check</h3> <p>After solving the Riccati or LMI for a given \(\gamma\), test:</p> \[M = \gamma^2 I - B_1^\top P B_1.\] <p>If \(M \succ 0\), then the Isaacs condition holds and<br/> \(\min\max = \max\min\).<br/> If \(M\) has nonpositive eigenvalues, the saddle-point equivalence fails.</p> \[x_k^T P x_k = \min_{u_k} \max_{w_k} \left\{ x_k^T Q x_k + u_k^T R u_k - \gamma^2 w_k^T w_k + x_{k+1}^T P x_{k+1} \right\}\] <hr/> <h1 id="3-solving-the-game-finding-the-moves">3. Solving the Game (finding the moves)</h1> <p>We denote the expression inside as \(L\):</p> \[L = x_k^T Q x_k + u_k^T R u_k - \gamma^2 w_k^T w_k + (A x_k + B_1 w_k + B_2 u_k)^T P (A x_k + B_1 w_k + B_2 u_k)\] <h2 id="step-1-the-disturbances-move-maximize-l">Step 1: The Disturbance’s Move (maximize \(L\))</h2> \[\frac{\partial L}{\partial w_k} = -2\gamma^2 w_k + 2 B_1^T P (A x_k + B_1 w_k + B_2 u_k) = 0\] <p>Solve for \(w_k^*\): \(w_k^* = (\gamma^2 I - B_1^T P B_1)^{-1} B_1^T P (A x_k + B_2 u_k)\)</p> <p><strong>Key insight:</strong><br/> This is the <strong>worst-case disturbance</strong>, dependent on both \(x_k\) and \(u_k\).</p> <hr/> <h2 id="step-2-the-controllers-move-minimize-l">Step 2: The Controller’s Move (minimize \(L\))</h2> <p>\(\frac{\partial L}{\partial u_k} = 2R u_k + 2 B_2^T P (A x_k + B_1 w_k + B_2 u_k) = 0\) Solve for \(u_k^*\): \(u_k^* = -(R + B_2^T P B_2)^{-1} B_2^T P (A x_k + B_1 w_k)\)</p> <p><strong>Problem:</strong> \(u_k^*\) depends on \(w_k\), which is not known in real time.<br/> <strong>Solution:</strong> Assume the controller uses state feedback: \(u_k = -K x_k\) and the disturbance knows this strategy. (This is equivalent to the max-min formulation.)</p> <ul> <li>Note: we still don’t know how this assumption should be changed when \(u_k\) must subject to some strict causality on \(x_k\).</li> </ul> <hr/> <h1 id="4-the-result-the-h_infty-riccati-equation">4. The Result: The \(H_\infty\) Riccati Equation</h1> <p>We plug the worst-case disturbance \(w_k^*\) into \(L\), then find the \(u_k\) minimizing the result.<br/> After algebraic simplification (as in Doyle et al.), the Bellman equation yields the <strong>Discrete-Time Algebraic Riccati Equation (DARE)</strong> for \(H_\infty\) control:</p> \[P = Q + A^T P A - A^T P \begin{bmatrix} B_1 &amp; B_2 \end{bmatrix} \left( R_{H_\infty} + \begin{bmatrix} B_1 &amp; B_2 \end{bmatrix}^T P \begin{bmatrix} B_1 &amp; B_2 \end{bmatrix} \right)^{-1} \begin{bmatrix} B_1 &amp; B_2 \end{bmatrix}^T P A\] <p>where \(R_{H_\infty} = \begin{bmatrix} -\gamma^2 I &amp; 0 \\ 0 &amp; R \end{bmatrix}.\)</p> <hr/> <h1 id="5-lqr-vs-h_infty-a-simple-comparison">5. LQR vs. \(H_\infty\) (a simple comparison)</h1> <table> <thead> <tr> <th style="text-align: center">Control Type</th> <th style="text-align: center">Riccati Equation</th> <th style="text-align: center">Interpretation</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><strong>LQR (H₂)</strong> <br/><br/></td> <td style="text-align: center"><br/>\(P = Q + A^T P A - A^T P B_2 (R + B_2^T P B_2)^{-1} B_2^T P A\)<br/><br/></td> <td style="text-align: center"><br/>Controller minimizes its own cost<br/><br/></td> </tr> <tr> <td style="text-align: center"><strong>\(H_\infty\)</strong> <br/><br/></td> <td style="text-align: center"><br/>\(P = Q + A^T P A - A^T P B_{\text{all}} R_{\text{all}}^{-1} B_{\text{all}}^T P A\)<br/><br/></td> <td style="text-align: center"><br/>Controller competes with disturbance (B_1 w)<br/><br/></td> </tr> </tbody> </table> <p>Here, \(B_{\text{all}}\) and \(R_{\text{all}}\) combine the control and disturbance effects:</p> \[B_{\text{all}} = [B_1 \; B_2], \quad R_{\text{all}} = \begin{bmatrix} -\gamma^2 I &amp; 0 \\ 0 &amp; R \end{bmatrix}\] <p>If there is \(Y\) and \(X\succ 0\) such that the following LMI holds:</p> \[\begin{bmatrix} X &amp; (A X + B_2 Y)^{\top} &amp; B_1^{\top} &amp; (C_1 X + D_{12} Y)^{\top} \\ A X + B_2 Y &amp; X &amp; 0 &amp; 0 \\ B_1 &amp; 0 &amp; \gamma^2 I &amp; 0 \\ C_1 X + D_{12} Y &amp; 0 &amp; 0 &amp; I \end{bmatrix} \succ 0,\] <p>then the following state-feedback control policy \(K = Y X^{-1}\) ensures that \(J &lt; 0\) for all \(w \in \ell_2\), i.e. \(\|T_{zw}\|_\infty&lt; \gamma\).</p> <hr/> <h2 id="interpretation">Interpretation</h2> <ul> <li>The \(-\gamma^2 I\) term acts as a <strong>negative cost</strong>, representing the <strong>disturbance player’s gain</strong>.</li> <li>The controller must design feedback strong enough to <strong>counteract</strong> it, ensuring that the closed-loop system remains stable.</li> <li>The \(H_\infty\) Riccati equation is just the LQR Riccati equation modified to include the <strong>negative cost</strong> of the maximizing disturbance player, scaled by \(\frac{1}{\gamma^2}\).</li> </ul> <hr/>]]></content><author><name></name></author><category term="control"/><category term="math"/><category term="optimization"/><summary type="html"><![CDATA[A simple, time-domain derivation from the game perspective provides an intuitive understanding of where the $$H_\infty$$ Riccati equation comes from. It's a direct application of dynamic programming, similar to the LQR derivation.]]></summary></entry><entry><title type="html">Semidefinite Programming Quick notes</title><link href="https://bobb1ranger.github.io/blog/2025/SDP/" rel="alternate" type="text/html" title="Semidefinite Programming Quick notes"/><published>2025-11-09T19:12:00+00:00</published><updated>2025-11-09T19:12:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/SDP</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/SDP/"><![CDATA[<h1 id="conic-programming">Conic Programming</h1> <p>Conic optimization is a subfield of convex optimization that studies problems consisting of minimizing a convex function over the intersection of an affine subspace and a convex cone.</p> <p><strong><em>convex cone</em></strong>: In linear algebra, a cone—sometimes called a linear cone to distinguish it from other sorts of cones—is a subset of a real vector space that is closed under positive scalar multiplication That is \(C\) is a cone if \(x\in C\) implies \(s x \in C\) for every \(s&gt;0\).</p> <ul> <li>A convex cone is a cone that is also closed under addition, or, equivalently, a subset of a vector space that is closed under linear combinations with positive coefficients. It follows that convex cones are convex sets.</li> <li>The conical hull of a set \(C\) is defined as the smallest convex cone that contains \(C \cup \{0\}\). Therefore, it need not be the smallest cone that contains \(C \cup \{0\}\).</li> </ul> <p>The class of conic optimization problems includes some of the most well known classes of convex optimization problems, namely linear and semidefinite programming.</p> <hr/> <h2 id="definition">Definition</h2> <p>Given a real vector space X, a convex, real-valued function:</p> <p>\(f \colon C \mapsto \mathbb{R}\) defined on a convex cone \(C \subset X\), and an affine subspace \(\cal{H}\) defined by a set of affine constraints \(h_i (x) =0\), a conic optimization problem is to find the point \(x \in C \cap \cal{H}\) for which the number \(f(x)\) is minimized.</p> <ul> <li>Example of \(C\) include the positive orthant \(\mathbb{R}_+^n \colon =\{x \in \mathbb{R}^n \colon x \geq 0\}\), positive semidefinite matrices \(\mathbb{S}_+^n\).</li> <li>Often \(f\) is a linear function, in which case the conic optimization problem reduces to a linear program, a semidefinite program, respectively.</li> </ul> <hr/> <h1 id="semidefinite-programming">Semidefinite Programming</h1> <p><strong>Semidefinite programming (SDP)</strong> is a subfield of mathematical programming concerned with the optimization of a linear objective function (a user-specified function that the user wants to minimize or maximize) over the intersection of <strong>the cone of positive semidefinite matrices</strong> with <strong>an affine space</strong>, i.e., a spectrahedron.</p> <ul> <li>A spectrahedron is a shape that can be represented as a linear matrix inequality.</li> </ul> <hr/> <h2 id="initial-motivation">Initial motivation</h2> <p>In semidefinite programming, we use real-valued vectors and are allowed to take the dot product of vectors with semi-definiteness constraints on matrix variables.</p> \[\min_{x^1, ... , x^n \in \mathbb{R}^n} \sum_{i,j \in [n]} c_{i,j} (x^i \cdot x^j)\] \[\text{subject to } \sum_{i,j \in [n]} a_{i,j,k} (x^i \cdot x^j) \leq b_k \quad \forall k.\] <h2 id="equivalent-formulations">Equivalent formulations</h2> <p>Use the gram matrix of some vectors \(X \colon = [x^i \cdot x^j]\). We can rewrite the mathematical program given in the previous section equivalently as:</p> \[\min_{X\in \mathbb{S}^n} \langle C, X\rangle\] \[\text{subject to } X \succeq \mathbf{0}, \langle A_k, X\rangle \leq b_k \quad \forall k.\] <h2 id="duality-theory">Duality Theory</h2> <p>Analogously to linear programming, given a general SDP of the form</p> \[\min_{X\in \mathbb{S}^n} \langle C, X\rangle\] \[\text{subject to } X \succeq \mathbf{0}, \langle A_k, X\rangle \leq b_k \quad \forall k.\] <p>(The primal problem of P-SDP), we define the dual semidefinite program (D-SDP) as</p> \[\max_{y \in \mathbb{R}^n} b^\top y\] \[\text{subject to } \sum_{i = 1}^m y_i A_i \preceq C.\] <ul> <li>Weak duality always holds, which means any feasible solution to the dual SDP lower-bounds the primal SDP value, and conversely, any feasible solution to the primal SDP upper-bounds the dual SDP value.</li> <li>Strong Duality doens’t hold always, which differs from the LP(linear programming). A sufficient condition for strong duality is the <strong>Slater’s condition</strong>.</li> </ul> <h2 id="algorithms-to-solve-sdp">Algorithms to solve SDP</h2>]]></content><author><name></name></author><category term="optimization-lectures"/><category term="math"/><category term="vector_space"/><category term="optimization"/><summary type="html"><![CDATA[a quick note of related math preliminaries on the semidefinite programming]]></summary></entry><entry><title type="html">Lecture note - Inexact Fixed-Point Iterations for Min-Max Problems</title><link href="https://bobb1ranger.github.io/blog/2025/Stephen-Wright-Talk/" rel="alternate" type="text/html" title="Lecture note - Inexact Fixed-Point Iterations for Min-Max Problems"/><published>2025-10-24T20:30:00+00:00</published><updated>2025-10-24T20:30:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/Stephen-Wright-Talk</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/Stephen-Wright-Talk/"><![CDATA[<p>I really enjoyed this insightful talk and the speaker’s style of giving lectures.</p> <p>Disclaimer: This is a summary of Stephen Wright’s talk at University of Minnesota on October 24, 2025. The summary was prepared by an attendee and may not capture all details accurately. For complete understanding, please refer to the original <a href="https://arxiv.org/abs/2402.05071">work</a> or contact the <a href="https://wrightstephen.github.io/sw_proj//">speaker</a>. This note is still under work.</p> <h1 id="abstract">Abstract:</h1> <p>We consider constrained, L-smooth, nonconvex-nonconcave min-max problems either satisfying \(\rho\) -cohypomonotonicity or admitting a solution to the \(\rho\)-weakly Minty Variational Inequality (MVI), where larger values of the parameter \(\rho\) &gt; 0 correspond to a greater degree of nonconvexity. Relevant problem classes include two player reinforcement learning and interaction-dominant min-max problems. We proposed inexact variants of Halpern and Krasnoselskii-Mann (KM) iterations and show that they can tolerate more nonconvexity than previously proved. We also provide stochastic algorithms which can tolerate the same amount of nonconvexity. Complexity results are proved in both cases. (Our improvements come from harnessing the recently proposed “conic nonexpansiveness” property of operators.) Finally, we provide a refined analysis for inexact Halpern iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator. This talk represents joint work with Ahmet Alacaoglu and Donghwan Kim.</p> <h1 id="introduction">Introduction</h1> <p>Consider the problem</p> \[\begin{equation}\label{eq:minmaxprob} \min_{u\in U} \max_{v \in V} f(u,v) \end{equation}\] <p>where \(U\subseteq \mathbb{R}^m\) and \(V\subseteq \mathbb{R}^n\) are closed convex sets admitting efficient projection operators and \(f:\mathbb{R}^m \times \mathbb{R}^n \mapsto \mathbb{R}\) is a function such that \(\nabla_u f(u,v)\) and \(\nabla_v f(u,v)\) are Lipschitz continuous. The general setting where \(f(u,v)\) can be nonconvex-nonconcave is extremely relevant in machine learning, with applications in generative adversarial networks (GAN).</p> <p>The following non-monotone inclusion (NMI) problem generalizes \eqref{eq:minmaxprob}.</p> \[\text{Find }x^* \in \mathbb{R}^d \text{ such that } 0 \in F(x^*) + G(x^ *),\] <p>where \(F\) is possibly nonmonotone but \(L\)-Lipschitz and \(G: \mathbb{R}^d \rightrightarrows \mathbb{R}^d\) is maximally monotone. Note that we recover Problem \eqref{eq:minmaxprob} by letting:</p> \[x = \begin{bmatrix} u \\ v \end{bmatrix}, \ F(x) = \begin{bmatrix} \nabla_u f(u,v) \\ -\nabla_v f(u,v) \end{bmatrix} \text{ and } G(x) = \begin{bmatrix} \partial{\iota_U} \\ \partial{\iota_V} \end{bmatrix},\] <p>where \(\iota_U\) is the indicator functionfor set \(U\). \(G\) can be understood as simply the subgradient of the convex constraint \(u \in U\) and \(v \in V\). For example, in 1-d case \(U \doteq [0,1]\).</p> \[\iota_U(u) = \begin{cases} 0, &amp; u \in U \\ +\infty, &amp; u \notin U\\ \end{cases}, \text{ and } \partial{\iota_U(u)} = \begin{cases} \{0\}, &amp; u \in (0,1) \\ (-\infty,0], &amp; u = 0\\ [0,\infty), &amp; u = 1\\ \emptyset, &amp; u \notin U \end{cases}.\] <p>The main assumption made is that \(F + G\) is \(\rho\)-cohypomonotone.</p> <h1 id="preliminaries">Preliminaries</h1> <h2 id="mvi-condition-and-weak-mvi">MVI condition and Weak MVI:</h2> <p>The Minty Variational Inequality (MVI) condition states that there exists a solution \(x^*\) such that</p> <ul> <li>Minmax is a special case of NMI</li> <li>Weak MVI holds for y =x*</li> </ul> <h2 id="resolvent">Resolvent:</h2> <p>Resolvent of an operator \(H: \mathbb{R}^d \rightrightarrows \mathbb{R}^d\) is defined as \(J_H := (I + H)^{-1}\).</p> <h2 id="non-expansive-operators">Non-expansive operators</h2> <p>Construct firmly non-expansive operator from non-expansive operator</p> <hr/> <h1 id="algorithm">Algorithm</h1> <h2 id="1-restate-as-a-fixed-point-problem">(1) Restate as a fixed-point problem</h2> <p>\(0 \in (F +G)(x^*)\) is equivalent to</p> \[\begin{align} &amp; x^* = J_{\eta(F + G)}(x^*) = (I + \eta (F + G))^{-1} (x^*) \\ \text{or } &amp; x^* = (1- \alpha) x^* + \alpha J_{\eta(F + G)}(x^*)\\ \end{align}\] <h2 id="2-outer-loop">(2) Outer Loop</h2> <p>Apply a fixed-point iteration (KM or Halpern) that allows for <strong>inexact</strong> evaluation of the resolvent.</p> <ul> <li>Inexactness: \(\|\tilde{J}_{\eta(F + G)} (x_k) - J_{\eta(F + G)} (x_k)\|^2 \leq \epsilon_k^2\). Choice of \(\epsilon_k\) is crucial.</li> <li>Halpern: \(x_{k+1} = \beta_k x_0 + (1-\beta_k)((1- \alpha) x_k + \alpha \tilde{J}_{\eta(F + G)}(x_k))\)</li> <li>KM: \(x_{k+1} = \beta_k x_k + (1-\beta_k)\tilde{J}_{\eta(F + G)}(x_k)\).</li> </ul> <h2 id="3-inner-loop">(3) Inner Loop</h2> <p>Use an optimal first-order algorithm to calculate \(\tilde{J}_{\eta(F + G)}(x_k)\), choosing \(\eta\) to ensure that this subproblem is strongly convex- strongly concave. (Stochastic) extragradient can solve this problem with optimal first-order complexity.</p> <h3 id="details">Details:</h3> <ul> <li>Sublinear convergence rate when there is no exact gradient</li> <li>FBF Forward-Backward-Forward Algorithm</li> </ul> <p>Stochastic Access F via an unbiased oracle Fhat</p> <p>Solve z \in (I _eta(F+G))(z) J \etaF = ( I + \eta F)^(-1) (u,v) = Argmin max f(u’ , v’) + ½\eta (u’ - u)^2 - ½\eta(v’-v)^2</p>]]></content><author><name></name></author><category term="optimization-lectures"/><category term="math"/><category term="convex_optimization"/><category term="duality"/><summary type="html"><![CDATA[Keypoints from Stephen Wright's Talk at University of Minnesota]]></summary></entry><entry><title type="html">Large Models and Training Knowledge Points</title><link href="https://bobb1ranger.github.io/blog/2025/Notes-Transformer/" rel="alternate" type="text/html" title="Large Models and Training Knowledge Points"/><published>2025-10-23T22:30:00+00:00</published><updated>2025-10-23T22:30:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/Notes-Transformer</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/Notes-Transformer/"><![CDATA[<p><a href="/assets/pdf/Transformer_Notes.pdf">Open PDF</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Transformer_1-480.webp 480w,/assets/img/Transformer_1-800.webp 800w,/assets/img/Transformer_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Transformer_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The transformer architecture. </div> <h1 id="1-model-principles"><strong>1. Model Principles</strong></h1> <h2 id="11-transformer"><strong>1.1 Transformer</strong></h2> <h3 id="architecture-overview">Architecture Overview</h3> <p>The Transformer consists of an <strong>Encoder–Decoder</strong> structure, primarily composed of the following modules:</p> <ul> <li><strong>Encoder:</strong> <ul> <li>Multi-head self-attention mechanism</li> <li>Feedforward neural network</li> <li>Layer normalization</li> <li>Residual connection</li> </ul> </li> <li><strong>Decoder:</strong> <ul> <li>Masked multi-head self-attention</li> <li>Encoder–decoder attention</li> <li>Feedforward neural network</li> </ul> </li> </ul> <h3 id="encoder">Encoder</h3> <p><strong>Functional overview of each component:</strong></p> <ul> <li> <p><strong>Positional Encoding</strong> – absolute or relative position encoding enables sequence modeling. It maps the semantic and positional information of tokens into the <strong>same vector space</strong>, allowing self-attention to jointly process both types of information. This makes the attention mechanism directly exploit their joint similarity while keeping parameter count constant and improving training efficiency.<br/> \(z_i = x_i + p_i\)<br/> Here, \(x_i\) encodes semantic meaning and \(p_i\) encodes positional information.</p> </li> <li> <p><strong>Multi-head attention</strong> – the Multi-Head mechanism improves representational capacity by splitting parameters into multiple attention heads, allowing parallel feature extraction.</p> </li> <li><strong>Feedforward neural network (FNN)</strong> – applied after the attention mechanism to independently transform each token representation through nonlinear mappings:<br/> \(\text{FNN}(x) = W_2 \, \text{ReLU}(W_1 x + b_1) + b_2\) Example dimensions: \(d_{\text{model}} = 512\), \(d_{\text{ff}} = 2048\).<br/> For each token: <ul> <li> \[W_1: (2048 \times 512)\] </li> <li>\(W_2: (512 \times 2048)\)<br/> Used for:<br/> (a) learning nonlinear mappings,<br/> (b) extracting per-position features,<br/> (c) expanding and compressing feature space to enhance expressivity.</li> </ul> </li> <li> <p><strong>Layer normalization</strong></p> </li> <li><strong>Residual connection</strong></li> </ul> <h3 id="decoder">Decoder</h3> <p>The decoder has a similar structure to the encoder but introduces <strong>an extra attention layer</strong> to connect with encoder outputs.</p> <ol> <li> <p><strong>Masked multi-head self-attention:</strong><br/> Since text is generated sequentially, the model must satisfy the <strong>causal constraint</strong> — each token can only depend on previous tokens.<br/> This is achieved by masking future positions in the attention weights, enforcing <strong>auto-regressive generation</strong>.</p> </li> <li> <p><strong>Encoder–decoder attention:</strong><br/> Acts as a bridge between the <strong>encoder’s contextual representation</strong> and the <strong>decoder’s generated output</strong>.</p> <ul> <li><strong>Query (Q):</strong> from the decoder (currently generated sequence).</li> <li><strong>Key (K), Value (V):</strong> from the encoder (semantic representation of the input).</li> </ul> </li> </ol> <hr/> <h2 id="12-attention-mechanism"><strong>1.2 Attention Mechanism</strong></h2> <p><strong>Advantages:</strong></p> <ul> <li>Captures long-range dependencies (no vanishing gradient as in RNNs).</li> <li>Enables parallel computation (tokens processed independently).</li> <li>Attention weights are interpretable and visualizable.</li> <li>Handles variable-length input sequences.</li> </ul> <hr/> <h3 id="mathematical-foundation-qkv-matrices--softmax-normalization">Mathematical foundation (Q–K–V matrices + softmax normalization)</h3> <h4 id="qkv-matrix-computation">QKV Matrix Computation</h4> <p>Each input token is linearly projected to obtain Query (Q), Key (K), and Value (V):</p> \[Q = X W_Q, \quad K = X W_K, \quad V = X W_V\] <p>where<br/> \(X \in \mathbb{R}^{n \times d_{\text{model}}}\) \(n\) = sequence length, \(d_{\text{model}}\) = embedding dimension.</p> <ul> <li>\(W_{Q}\), \(W_{K}\), \(W_{V}\) are trainable projection matrices.</li> <li>\(d_k\) = attention dimension, usually set as \(d_k = \frac{d_{\text{model}}}{h}\).</li> </ul> <hr/> <h3 id="self-attention-mechanism">Self-Attention Mechanism</h3> <p>Compute attention weights as scaled dot-products between queries and keys, then use them to weight values:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V\] <p>The scaling factor \(\sqrt{d_k}\) prevents gradient vanishing/exploding during training.</p> <hr/> <h3 id="multi-head-attention">Multi-Head Attention</h3> \[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O\] <p>where each head is: \(\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\)</p> <ul> <li>\(h\): number of heads</li> <li>Each head has independent learnable matrices \(W_i^Q\), \(W_i^K\), \(W_i^V\).</li> <li>Outputs are concatenated and projected to \(d_{\text{model}}\).</li> <li>This allows the model to attend to different representation subspaces and features simultaneously.</li> </ul> <hr/> <h3 id="masked-self-attention">Masked Self-Attention</h3> <p>Used during generation to prevent tokens from attending to future positions:</p> \[\text{MaskedAttention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V\] <p>where the mask matrix \(M = [M_{ij}]\) satisfies: \(M_{ij} = \begin{cases} 0, &amp; i \ge j \\ -\infty, &amp; i &lt; j \end{cases}\)</p> <p>This ensures causal, left-to-right generation.</p> <hr/> <h1 id="2-bert-gpt-t5-architectures"><strong>2. BERT, GPT, T5 architectures</strong></h1> <p>Pretrained language models based on Transformers have achieved major breakthroughs in the field of natural language processing (NLP). Architecture Categories:</p> <ul> <li>Encoder models: contain only the encoder component; suitable for extracting contextual representations (e.g., BERT).</li> <li>Decoder models: contain only the decoder component; suitable for autoregressive generation tasks (e.g., GPT).</li> <li>Encoder–Decoder models: combine both components; suitable for sequence-to-sequence tasks (e.g., T5).</li> </ul> <h2 id="bert-bidirectional-encoder-representation-from-transformers">BERT: Bidirectional Encoder Representation from Transformers</h2> <ol> <li><strong>Encoder-only architecture</strong> — uses only Transformer encoder layers.</li> <li><strong>Pretraining process:</strong> conducted through unsupervised learning, using <strong>Masked Language Modeling (MLM)</strong> and <strong>Next Sentence Prediction (NSP)</strong> tasks. <ul> <li><strong>Masked Language Model (MLM):</strong> randomly masks certain words in the input text, and the model must predict the masked words.</li> <li><strong>Next Sentence Prediction (NSP):</strong> trains the model to determine whether two given sentences are consecutive in the original text, improving contextual understanding.</li> </ul> </li> <li><strong>Fine-tuning process:</strong> adapts the pretrained model to specific downstream tasks (e.g., text classification, QA).</li> </ol> <h2 id="gpt-generative-pretrained-transformer">GPT: Generative Pretrained Transformer</h2> <ol> <li><strong>Decoder-only architecture</strong> — uses a unidirectional autoregressive model composed solely of decoder layers.</li> <li><strong>Pretraining:</strong> uses the <strong>Causal Language Modeling (CLM)</strong> task — predicting the next token based on all preceding tokens.</li> <li><strong>Fine-tuning:</strong> primarily used for text generation tasks, such as dialogue generation and summarization.</li> </ol> <h2 id="t5-text-to-text-transfer-transformer">T5: Text-to-Text Transfer Transformer</h2> <ol> <li><strong>Full Encoder–Decoder architecture.</strong></li> <li><strong>Pretraining:</strong> <ul> <li><strong>Text infilling:</strong> similar to BERT’s MLM but can mask multiple consecutive tokens.</li> <li><strong>Denoising:</strong> randomly deletes segments or words in the input text, and the model must reconstruct the original text.</li> </ul> </li> <li><strong>Fine-tuning:</strong> applicable to a wide variety of text generation tasks, including summarization, translation, and text classification.</li> </ol> <hr/> <h1 id="3-pretraining"><strong>3. Pretraining</strong></h1> <p>Pretraining refers to learning language patterns on large-scale raw corpora (typically internet-scale data) through self-supervised tasks (e.g., next-word prediction or Masked LM). Its goal is to equip the model with general language understanding and generation capabilities, rather than solving a specific task.</p> <h2 id="data-scale-from-millions-to-trillions-of-tokens">Data Scale: From Millions to Trillions of Tokens</h2> <p>The amount of data required for pretraining mainly depends on the model size (number of parameters).<br/> Empirical rules come from the Scaling Laws (Kaplan et al., 2020): the larger the model, the more training tokens are needed in proportion; otherwise, the model becomes undertrained.</p> <table> <thead> <tr> <th>Model Size</th> <th>Representative Model</th> <th>Training Tokens (Empirical)</th> <th>Estimated Data Volume</th> </tr> </thead> <tbody> <tr> <td>~100M params</td> <td>Small language models (e.g., DistilGPT)</td> <td>10⁹ (1B)</td> <td>Tens of GB</td> </tr> <tr> <td>~1B params</td> <td>GPT-2 Small</td> <td>10¹⁰ (10B)</td> <td>Hundreds of GB</td> </tr> <tr> <td>~6B params</td> <td>GPT-J / GPT-NeoX</td> <td>10¹¹ (100B)</td> <td>TB scale</td> </tr> <tr> <td>~70B params</td> <td>LLaMA-2 / Falcon-70B</td> <td>2×10¹² (2T)</td> <td>Tens of TB</td> </tr> <tr> <td>~1T params</td> <td>GPT-4 level</td> <td>10¹³ (10T)</td> <td>Hundreds of TB</td> </tr> </tbody> </table> <ul> <li><strong>Empirical ratio rule:</strong> Ideal training tokens ≈ <strong>20×number of parameters (in tokens)</strong>. For example, a 70B model should ideally be trained on ~1.4T tokens.</li> </ul> <h2 id="training-time">Training Time</h2> <p>Assuming we use A100 GPUs (80GB) or H100 GPUs (80GB):</p> <table> <thead> <tr> <th>Model</th> <th>GPU Type</th> <th>Number of GPUs</th> <th>Data Volume (Tokens)</th> <th>Estimated Training Time</th> </tr> </thead> <tbody> <tr> <td>GPT-2 (1.5B)</td> <td>V100 × 32</td> <td>32 GPUs</td> <td>40B</td> <td>~1 week</td> </tr> <tr> <td>GPT-J (6B)</td> <td>A100 × 64</td> <td>64 GPUs</td> <td>400B</td> <td>~2-3 weeks</td> </tr> <tr> <td>LLaMA-2 70B</td> <td>A100 × 2048</td> <td>2048 GPUs</td> <td>2T</td> <td>~3-4 weeks</td> </tr> <tr> <td>GPT-4 level</td> <td>H100 × 10,000+</td> <td>Tens of thousands</td> <td>10T</td> <td>Several months</td> </tr> </tbody> </table> <h2 id="data-preparation-workload">Data Preparation Workload</h2> <p>Pretraining corpora usually come from a mix of sources:</p> <table> <thead> <tr> <th>Data Type</th> <th>Proportion</th> <th>Source</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>Web Text</td> <td>50-70%</td> <td>Common Crawl, Wikipedia</td> <td>Language diversity</td> </tr> <tr> <td>Books / Papers</td> <td>10-20%</td> <td>BooksCorpus, ArXiv</td> <td>Long context, logic</td> </tr> <tr> <td>Code</td> <td>10-20%</td> <td>GitHub, StackOverflow</td> <td>Program understanding</td> </tr> <tr> <td>Dialogue / QA</td> <td>5-10%</td> <td>OpenAssistant, Reddit</td> <td>Interactive structure</td> </tr> </tbody> </table> <p>Data cleaning often takes more time than training itself. These steps include:</p> <ul> <li>Deduplication</li> <li>Language detection</li> <li>Spam filtering</li> <li>Text normalization (tokenize, normalize)</li> </ul> <hr/> <h1 id="4-fine-tuning-and-instruction-alignment"><strong>4. Fine-Tuning and Instruction Alignment</strong></h1> <h2 id="41--sft-supervised-fine-tuning-and-continued-pretraining">4.1 SFT (Supervised Fine-Tuning) and Continued Pretraining</h2> <p><strong>Supervised Fine-Tuning (SFT)</strong> adapts a pretrained model to a specific task using labeled data or instruction-response pairs.</p> <h3 id="step-1--define-tasks-and-instructions">Step 1 – Define Tasks and Instructions</h3> <ul> <li><strong>Task definition:</strong> specify what the model should do (e.g., text classification, summarization, translation).</li> <li><strong>Instruction design:</strong> build prompt templates such as<br/> <em>“Classify the following text as positive or negative.”</em></li> </ul> <h3 id="step-2--collect-raw-data">Step 2 – Collect Raw Data</h3> <ul> <li>Use public datasets, domain-specific corpora, or crowdsourcing.</li> <li>Clean and normalize data (remove duplicates, fix encoding, etc.).</li> </ul> <h3 id="step-3--label-data">Step 3 – Label Data</h3> <ul> <li>Establish clear annotation rules for consistency.</li> <li>Apply human labeling or semi-automatic labeling tools.</li> </ul> <h3 id="step-4--construct-instruction-dataset">Step 4 – Construct Instruction Dataset</h3> <ul> <li>Split data into training / validation / test sets.</li> <li>Store in structured formats (JSON, CSV, etc.).</li> </ul> <h3 id="step-5--enhance-data-diversity">Step 5 – Enhance Data Diversity</h3> <ul> <li>Use synonym replacement or paraphrasing for augmentation.</li> <li>Cover multiple use-case scenarios.</li> </ul> <h3 id="step-6--evaluate-and-iterate">Step 6 – Evaluate and Iterate</h3> <ul> <li>Run small-scale evaluation to check performance.</li> <li>Refine prompts or add examples for better alignment.</li> </ul> <blockquote> <p>⚠ SFT may cause <strong>over-specialization</strong>—the model can lose generality and exploratory ability.<br/> A common remedy: mix <strong>instruction + pretraining data</strong>, and follow SFT with preference optimization (DPO / PPO / ORPO).</p> </blockquote> <hr/>]]></content><author><name></name></author><category term="LLM"/><category term="AI"/><category term="math"/><summary type="html"><![CDATA[A quick note on Transformer Model]]></summary></entry><entry><title type="html">Normed Vector Space</title><link href="https://bobb1ranger.github.io/blog/2025/Vector-Optimization/" rel="alternate" type="text/html" title="Normed Vector Space"/><published>2025-04-05T05:41:00+00:00</published><updated>2025-04-05T05:41:00+00:00</updated><id>https://bobb1ranger.github.io/blog/2025/Vector-Optimization</id><content type="html" xml:base="https://bobb1ranger.github.io/blog/2025/Vector-Optimization/"><![CDATA[<p>Given a complex-valued \(m \times n\) matrix \(A\), \(A^H\) denotes its hermitian conjugate. \(\ell_2^{m\times n}[Z]\) denotes the Hilbert space of sequences of \(m\times n\) complex-valued matrices, with inner product defined as</p> \[\langle H,G\rangle = \sum_{k=-\infty}^{\infty} \mathrm{trace}(G^H[k]H[k]).\] <p>The \(\ell_2^{m\times n}[Z]\) space can be decomposed as the direct sum of two spaces of sequences \(\ell_2^{m\times n}[Z^+] \oplus \ell_2^{m\times n}[Z^-]\) . The unilateral z-transform of \(G\in \ell_2^{m\times n}[Z+]\) is \(\hat{G}(z) = \sum_{k=0}^\infty G[k] z^{-k}\)</p> <p>\({\mathcal{H}_2^{m \times n}}^\perp\) is the set of functions \(\hat{G} : \mathbb{C} \to \mathbb{C}^{m \times n}\) such that</p> \[\frac{1}{2\pi j} \oint_{\mathcal{C}} \hat{G}(z) z^{n-1} \, dz = \mathbf{0}, \quad \forall n \geq 0\] <p>There is a unique decomposition into \(\mathcal{H}_2\) and \(\mathcal{H}_2^\perp\) for all matrix-valued functions analytic on the unit circle, by the Projection Theorem.</p> <p>\(\mathcal{RH}_2\) and \(\mathcal{RH}_2^\perp\) respectively denote the rational proper transfer function matrices in \(\mathcal{H}_2\) and \(\mathcal{H}_2^\perp\).</p>]]></content><author><name></name></author><category term="optimization-lectures"/><category term="math"/><category term="vector_space"/><category term="optimization"/><category term="duality"/><summary type="html"><![CDATA[a quick note from David Luenberger's book "Optimization by Vector Space Methods"]]></summary></entry></feed>